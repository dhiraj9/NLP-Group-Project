{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl = pd.read_csv('new_ctrl.csv')\n",
    "gpt = pd.read_csv('new_gpt.csv')\n",
    "gpt2 = pd.read_csv('new_gpt2.csv')\n",
    "gpt3 = pd.read_csv('new_gpt3.csv')\n",
    "instructgpt = pd.read_csv('new_instructgpt.csv')\n",
    "grover = pd.read_csv('new_grover.csv')\n",
    "xlm = pd.read_csv('new_xlm.csv')\n",
    "xlnet = pd.read_csv('new_xlnet.csv')\n",
    "\n",
    "pplm = pd.read_csv('new_pplm.csv')\n",
    "human = pd.read_csv('new_human.csv')\n",
    "fair = pd.read_csv('new_fair.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Generation', 'label'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_binary_t2 = pd.concat([human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), \n",
    "                              xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), ctrl.sample(1066), \n",
    "                              gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), \n",
    "                              pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), human.sample(1066), \n",
    "                              gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), gpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), \n",
    "                              pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066), \n",
    "                              gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), \n",
    "                              fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), \n",
    "                              grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), \n",
    "                              human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), \n",
    "                              xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), human.sample(1066), \n",
    "                              ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), \n",
    "                              pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), \n",
    "                          gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), \n",
    "                              fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), \n",
    "                              grover.sample(1066), xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), \n",
    "                              xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), human.sample(1066), ctrl.sample(1066), gpt.sample(1066), gpt2.sample(1066), gpt3.sample(1066), instructgpt.sample(1066), grover.sample(1066), \n",
    "                              xlm.sample(1066), xlnet.sample(1066), pplm.sample(1066), fair.sample(1066), human.sample(1066)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_binary_t1 = pd.concat([human,human, human, human, human, human, human, human, human, human, human, ctrl, ctrl, ctrl, ctrl, ctrl, ctrl,\n",
    "                             ctrl, ctrl, ctrl, ctrl, ctrl, gpt, gpt, gpt, gpt, gpt, gpt, gpt, gpt, gpt, gpt, gpt, gpt2, gpt2, gpt2, gpt2, gpt2, gpt2,\n",
    "                             gpt2, gpt2, gpt2, gpt2, gpt2, gpt3, gpt3, gpt3, gpt3, gpt3, gpt3, gpt3, gpt3, gpt3, gpt3, gpt3, instructgpt, instructgpt, instructgpt, instructgpt, instructgpt, instructgpt, instructgpt, instructgpt, instructgpt, instructgpt, instructgpt, grover, grover, grover, grover, grover, grover, grover, grover, grover, grover, grover, xlm, xlm, \n",
    "                             xlm, xlm, xlm, xlm, xlm, xlm, xlm, xlm, xlm, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, xlnet, \n",
    "                             pplm, pplm, pplm, pplm, pplm, pplm, pplm, pplm, pplm, pplm, pplm, fair, fair, fair, fair, fair, fair, fair, fair, fair, fair, fair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_binary_t1) == len(random_binary_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128986"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_binary_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Generation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>Al Pacino hunts Nazis in 'Hunters,' a series w...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>724</td>\n",
       "      <td>What it's like to be me by e. wes ely wes ely ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>Dutch government returns stolen 18th-century '...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>454</td>\n",
       "      <td>Sleep anxiety and daylight saving time can exa...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>636</td>\n",
       "      <td>Meet the 2019 CNN Hero of the Year (cnn) it's ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                         Generation  label\n",
       "61           61  Al Pacino hunts Nazis in 'Hunters,' a series w...  human\n",
       "724         724  What it's like to be me by e. wes ely wes ely ...  human\n",
       "498         498  Dutch government returns stolen 18th-century '...  human\n",
       "454         454  Sleep anxiety and daylight saving time can exa...  human\n",
       "636         636  Meet the 2019 CNN Hero of the Year (cnn) it's ...  human"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_binary_t2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = [np.ones(len(human)), \n",
    "             np.zeros(10 * 1066), \n",
    "             np.ones(len(ctrl)), \n",
    "             np.zeros(13 * 1066), \n",
    "             np.ones(len(gpt)), \n",
    "             np.zeros(10 * 1066),\n",
    "             np.ones(len(gpt2)),\n",
    "             np.zeros(11 * 1066),\n",
    "             np.ones(len(gpt3)),\n",
    "             np.zeros(11 * 1066),\n",
    "             np.ones(len(instructgpt)),\n",
    "             np.zeros(11 * 1066),\n",
    "             np.ones(len(grover)),\n",
    "             np.zeros(11 * 1066),\n",
    "             np.ones(len(xlm)),\n",
    "             np.zeros(11 * 1066),\n",
    "             np.ones(len(xlnet)),\n",
    "             np.zeros(11 * 1066),\n",
    "             np.ones(len(pplm)),\n",
    "             np.zeros(11 * 1066),\n",
    "             np.ones(len(fair))\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in new_label for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128986"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128986"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_binary_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128986"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_binary_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_random = pd.DataFrame({'T1': list(random_binary_t1['Generation']), 'T2': list(random_binary_t2['Generation']), 'class': flat_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_random.to_csv('binary_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_T1 = pd.concat([human.sample(1066), ctrl.sample(106), pplm.sample(106), gpt.sample(106), gpt2.sample(106), gpt3.sample(107), instructgpt.sample(107), grover.sample(107), \n",
    "                     xlm.sample(107), xlnet.sample(107), fair.sample(107), \n",
    "                       ctrl.sample(106), pplm.sample(106), gpt.sample(106), gpt2.sample(106), gpt3.sample(107), instructgpt.sample(107), grover.sample(107), \n",
    "                     xlm.sample(107), xlnet.sample(107), fair.sample(107), human.sample(1066)])\n",
    "balance_T2 = pd.concat([human, human, ctrl.sample(106), pplm.sample(106), gpt.sample(106), gpt2.sample(106), gpt3.sample(107), instructgpt.sample(107), grover.sample(107), \n",
    "                     xlm.sample(107), xlnet.sample(107), fair.sample(107), \n",
    "                       ctrl.sample(106), pplm.sample(106), gpt.sample(106), gpt2.sample(106), gpt3.sample(107), instructgpt.sample(107), grover.sample(107), \n",
    "                     xlm.sample(107), xlnet.sample(107), fair.sample(107)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Generation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Latest Headlines on CNN Business the great shu...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>China wants to take a victory lap over its han...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Coronavirus disinformation creates challenges ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>China coronavirus: Eating wild animals made il...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>China's economy could shrink for the first tim...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         Generation  label\n",
       "0           0  Latest Headlines on CNN Business the great shu...  human\n",
       "1           1  China wants to take a victory lap over its han...  human\n",
       "2           2  Coronavirus disinformation creates challenges ...  human\n",
       "3           3  China coronavirus: Eating wild animals made il...  human\n",
       "4           4  China's economy could shrink for the first tim...  human"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_T2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_label = [np.ones(1066),\n",
    "                 np.zeros(1066), \n",
    "                 np.ones(1066), \n",
    "                np.zeros(1066)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_label = [item for sublist in balance_label for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_p1 = pd.DataFrame({'T1': list(balance_T1['Generation']), 'T2': list(balance_T2['Generation']), 'class': balance_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_p1.to_csv('balanced_p1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "\n",
    "# # Define input file (text corpus) and output file (for the trained model)\n",
    "# input_file = 'data.txt'\n",
    "# output_file = 'model'\n",
    "\n",
    "# # Train Skipgram word embeddings with FastText\n",
    "# model = fasttext.train_unsupervised(input_file, model='skipgram', dim=100, epoch=5, lr=0.05, minCount=5)\n",
    "\n",
    "# # Save the trained model to a file\n",
    "# model.save_model(output_file + '.bin')\n",
    "\n",
    "# # Load the trained model from the file\n",
    "# loaded_model = fasttext.load_model(output_file + '.bin')\n",
    "\n",
    "# # Get word embeddings for specific words\n",
    "# word = 'example'\n",
    "# word_vector = loaded_model.get_word_vector(word)\n",
    "# print('Word vector for \"{}\":'.format(word), word_vector)\n",
    "\n",
    "# # Find similar words based on cosine similarity\n",
    "# similar_words = loaded_model.get_nearest_neighbors(word, k=10)\n",
    "# print('Top 10 similar words to \"{}\":'.format(word), similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, SimpleRNN, Dense, Attention, Concatenate\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# # Model configuration\n",
    "# input_sequence_length = 100\n",
    "# output_sequence_length = 100\n",
    "# embedding_dim = 128\n",
    "# rnn_units = 256\n",
    "\n",
    "# # Encoder\n",
    "# encoder_inputs = Input(shape=(input_sequence_length, embedding_dim), name=\"encoder_inputs\")\n",
    "# encoder_rnn = SimpleRNN(rnn_units, return_sequences=True, return_state=True, name=\"encoder_rnn\")\n",
    "# encoder_outputs, encoder_state = encoder_rnn(encoder_inputs)\n",
    "\n",
    "# # Decoder\n",
    "# decoder_inputs = Input(shape=(output_sequence_length, embedding_dim), name=\"decoder_inputs\")\n",
    "# decoder_rnn = SimpleRNN(rnn_units, return_sequences=True, return_state=True, name=\"decoder_rnn\")\n",
    "# decoder_outputs, _ = decoder_rnn(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "# # Attention\n",
    "# attention_layer = Attention(name=\"attention_layer\")\n",
    "# attention_output = attention_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# # Concatenate decoder outputs and attention output\n",
    "# concat_layer = Concatenate(axis=-1, name=\"concat_layer\")\n",
    "# decoder_attention_concat = concat_layer([decoder_outputs, attention_output])\n",
    "\n",
    "# # Dense layer for final output\n",
    "# output_layer = Dense(embedding_dim, activation=\"softmax\", name=\"output_layer\")\n",
    "# outputs = output_layer(decoder_attention_concat)\n",
    "\n",
    "# # Model\n",
    "# model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # Model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Binary Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    logistic_model = LogisticRegression()\n",
    "    parameters = { 'C' : [0.01, 0.1, 1, 3, 10],\n",
    "                   'penalty' : ['l2', 'elasticnet']\n",
    "                 }\n",
    "\n",
    "    cross_validation = 3\n",
    "    scoring_metric = \"f1\"\n",
    "\n",
    "#     clf = GridSearchCV(logistic_model,\n",
    "#                                      parameters,\n",
    "#                                      cv = cross_validation,\n",
    "#                                      scoring = scoring_metric,\n",
    "#                                      return_train_score=True)\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "#     fit = clf.fit(train_vector,y_train)\n",
    "#     print('Best Params ', clf.best_params_)\n",
    "    parameters = { 'C' : 10,\n",
    "               'penalty' : 'l2'\n",
    "     }\n",
    "\n",
    "    clf = LogisticRegression(C = parameters['C'], penalty = parameters['penalty'])\n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_p1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_random.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    clf = BernoulliNB()    \n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    randomforest_model = RandomForestClassifier()\n",
    "\n",
    "    parameters = { 'max_depth' : [10, 20, 30],\n",
    "                   'n_estimators' : [90, 150, 180],\n",
    "                   'max_samples' : [0.6, 0.8]\n",
    "     }\n",
    "\n",
    "    cross_validation = 3\n",
    "    scoring_metric = \"f1\"\n",
    "#     clf = GridSearchCV(randomforest_model, \n",
    "#                                          parameters,\n",
    "#                                          cv = cross_validation,\n",
    "#                                          scoring = scoring_metric,\n",
    "#                                          return_train_score=True)\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "#     fit = clf.fit(train_vector,y_train)\n",
    "#     print('Best Params ', clf.best_params_)\n",
    "    parameters = {\n",
    "     'max_depth' : 30,\n",
    "     'n_estimators' : 150,\n",
    "     'max_samples' : 0.6\n",
    "    }\n",
    "    clf = RandomForestClassifier(max_depth = parameters['max_depth'],\n",
    "                                            max_samples = parameters['max_samples'],\n",
    "                                            n_estimators = parameters['n_estimators'])\n",
    "\n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    xgboost_model = xgboost.XGBClassifier()\n",
    "\n",
    "#     parameters = { 'max_depth' : [10, 20, 30],\n",
    "#                    'n_estimators' : [90, 150, 180],\n",
    "#                    'min_child_weight' : [1, 5, 10 ]\n",
    "#      }\n",
    "\n",
    "    cross_validation = 3\n",
    "    scoring_metric = \"f1\"\n",
    "#     clf = GridSearchCV(xgboost_model, \n",
    "#                                     parameters,\n",
    "#                                     cv = cross_validation,\n",
    "#                                     scoring = scoring_metric,\n",
    "#                                     return_train_score=True)\n",
    "\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "#     fit = clf.fit(train_vector,y_train)\n",
    "#     print('Best Params ', clf.best_params_)\n",
    "    parameters = { 'max_depth' : 20,\n",
    "                   'min_child_weight' : 5,\n",
    "                   'n_estimators' : 180\n",
    "     }\n",
    "\n",
    "    clf = xgboost.XGBClassifier(max_depth = parameters['max_depth'],\n",
    "                                          min_child_weight = parameters['min_child_weight'],\n",
    "                                          n_estimators = parameters['n_estimators'])\n",
    "\n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.634232121922626\n",
      "F1: 0.6337483898668956\n",
      "Recall:  0.6341903882310255\n",
      "Precision:  0.6348799814335919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2166    1.0\n",
       " 4236    0.0\n",
       " 2002    0.0\n",
       " 512     1.0\n",
       " 3241    0.0\n",
       "        ... \n",
       " 747     1.0\n",
       " 535     1.0\n",
       " 311     1.0\n",
       " 750     1.0\n",
       " 2122    0.0\n",
       " Name: class, Length: 853, dtype: float64,\n",
       " array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9255108767303889\n",
      "F1: 0.6782046559182194\n",
      "Recall:  0.6299710781969565\n",
      "Precision:  0.8419981418086167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99756     0.0\n",
       " 13680     0.0\n",
       " 693       1.0\n",
       " 53068     0.0\n",
       " 114921    0.0\n",
       "          ... \n",
       " 123459    0.0\n",
       " 85061     0.0\n",
       " 126883    0.0\n",
       " 106214    0.0\n",
       " 10654     0.0\n",
       " Name: class, Length: 25789, dtype: float64,\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    svm_model = SVC()\n",
    "\n",
    "    parameters = { 'kernel' : ['poly', 'rbf', 'sigmoid'],\n",
    "                   'max_iter' : [20, 50, 100]\n",
    "     }\n",
    "\n",
    "    cross_validation = 3\n",
    "    scoring_metric = \"f1\"\n",
    "    clf = GridSearchCV(svm_model, \n",
    "                                parameters,\n",
    "                                cv = cross_validation,\n",
    "                                scoring = scoring_metric,\n",
    "                                return_train_score=True)\n",
    "\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    print('Best Params ', clf.best_params_)\n",
    "#     parameters = {\n",
    "#          'kernel' : 'sigmoid',\n",
    "#          'max_iter' : 50,\n",
    "#         }\n",
    "\n",
    "#     clf = SVC(kernel = parameters['kernel'],\n",
    "#     max_iter = parameters['max_iter'], probability = True)\n",
    "#     fit = clf.fit(train_vector,y_train)\n",
    "#     pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "#     Accuracy = accuracy_score(y_test,pred)\n",
    "#     F1 = f1_score(y_test, pred, average='macro')\n",
    "#     print(\"Accuracy:\", Accuracy)\n",
    "#     print('F1:', F1)\n",
    "    \n",
    "#     rec = recall_score(y_test, pred, average='macro')\n",
    "#     print('Recall: ', rec)\n",
    "#     prec = precision_score(y_test, pred, average='macro')\n",
    "#     print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "#     return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params  {'kernel': 'sigmoid', 'max_iter': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8978246539222149\n",
      "F1: 0.4812687281363775\n",
      "Recall:  0.4980176354942982\n",
      "Precision:  0.487224129216415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99756     0.0\n",
       " 13680     0.0\n",
       " 693       1.0\n",
       " 53068     0.0\n",
       " 114921    0.0\n",
       "          ... \n",
       " 123459    0.0\n",
       " 85061     0.0\n",
       " 126883    0.0\n",
       " 106214    0.0\n",
       " 10654     0.0\n",
       " Name: class, Length: 25789, dtype: float64,\n",
       " array([0., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    logistic_model = LogisticRegression()\n",
    "    parameters = { 'C' : [0.01, 0.1, 1, 3, 10],\n",
    "                   'penalty' : ['l2', 'elasticnet']\n",
    "                 }\n",
    "\n",
    "    cross_validation = 3\n",
    "    scoring_metric = \"f1\"\n",
    "\n",
    "#     clf = GridSearchCV(logistic_model,\n",
    "#                                      parameters,\n",
    "#                                      cv = cross_validation,\n",
    "#                                      scoring = scoring_metric,\n",
    "#                                      return_train_score=True)\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "#     fit = clf.fit(train_vector,y_train)\n",
    "#     print('Best Params ', clf.best_params_)\n",
    "    parameters = { 'C' : 0.01,\n",
    "               'penalty' : 'l2'\n",
    "     }\n",
    "\n",
    "    clf = LogisticRegression(C = parameters['C'], penalty = parameters['penalty'])\n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.41258578        nan 0.40547247        nan 0.39693972        nan\n",
      " 0.38498623        nan 0.36636151        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.50403802        nan 0.61296231        nan 0.80437185        nan\n",
      " 0.85992428        nan 0.91508212        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params  {'C': 0.01, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8888824551244934\n",
      "F1: 0.470586432052972\n",
      "Recall:  0.5\n",
      "Precision:  0.4444412275622467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2269     0.0\n",
       " 29062    0.0\n",
       " 21144    0.0\n",
       " 64101    1.0\n",
       " 46390    0.0\n",
       "         ... \n",
       " 46736    0.0\n",
       " 47506    0.0\n",
       " 49566    0.0\n",
       " 186      1.0\n",
       " 6185     0.0\n",
       " Name: class, Length: 17270, dtype: float64,\n",
       " array([0., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    svm_model = SVC()\n",
    "\n",
    "    parameters = { 'kernel' : ['poly', 'rbf', 'sigmoid'],\n",
    "                   'max_iter' : [20, 50, 100]\n",
    "     }\n",
    "\n",
    "    cross_validation = 3\n",
    "    scoring_metric = \"f1\"\n",
    "#     clf = GridSearchCV(svm_model, \n",
    "#                                 parameters,\n",
    "#                                 cv = cross_validation,\n",
    "#                                 scoring = scoring_metric,\n",
    "#                                 return_train_score=True)\n",
    "\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "#     fit = clf.fit(train_vector,y_train)\n",
    "#     print('Best Params ', clf.best_params_)\n",
    "    parameters = {\n",
    "         'kernel' : 'sigmoid',\n",
    "         'max_iter' : 50,\n",
    "        }\n",
    "\n",
    "    clf = SVC(kernel = parameters['kernel'],\n",
    "    max_iter = parameters['max_iter'], probability = True)\n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5205158264947245\n",
      "F1: 0.4568697506791522\n",
      "Recall:  0.5209178568679839\n",
      "Precision:  0.5394930770348536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2166    1.0\n",
       " 4236    0.0\n",
       " 2001    0.0\n",
       " 512     1.0\n",
       " 3240    0.0\n",
       "        ... \n",
       " 747     1.0\n",
       " 535     1.0\n",
       " 311     1.0\n",
       " 750     1.0\n",
       " 2121    0.0\n",
       " Name: class, Length: 853, dtype: float64,\n",
       " array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1.]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gdhir\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=50).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7527504342790967\n",
      "F1: 0.49311776678147934\n",
      "Recall:  0.49637916899493656\n",
      "Precision:  0.4975024988591847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2269     0.0\n",
       " 29062    0.0\n",
       " 21144    0.0\n",
       " 64101    1.0\n",
       " 46390    0.0\n",
       "         ... \n",
       " 46736    0.0\n",
       " 47506    0.0\n",
       " 49566    0.0\n",
       " 186      1.0\n",
       " 6185     0.0\n",
       " Name: class, Length: 17270, dtype: float64,\n",
       " array([1., 0., 0., ..., 1., 0., 0.]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    clf = BernoulliNB()    \n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36107854630715125\n",
      "F1: 0.35699219209250543\n",
      "Recall:  0.36098558564501765\n",
      "Precision:  0.3574143200297723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2166    1.0\n",
       " 4236    0.0\n",
       " 2001    0.0\n",
       " 512     1.0\n",
       " 3240    0.0\n",
       "        ... \n",
       " 747     1.0\n",
       " 535     1.0\n",
       " 311     1.0\n",
       " 750     1.0\n",
       " 2121    0.0\n",
       " Name: class, Length: 853, dtype: float64,\n",
       " array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 1.]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6357845975680371\n",
      "F1: 0.47428519913370065\n",
      "Recall:  0.5181301915921306\n",
      "Precision:  0.5080427305831513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2269     0.0\n",
       " 29062    0.0\n",
       " 21144    0.0\n",
       " 64101    1.0\n",
       " 46390    0.0\n",
       "         ... \n",
       " 46736    0.0\n",
       " 47506    0.0\n",
       " 49566    0.0\n",
       " 186      1.0\n",
       " 6185     0.0\n",
       " Name: class, Length: 17270, dtype: float64,\n",
       " array([0., 1., 0., ..., 0., 1., 0.]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data, label):\n",
    "            \n",
    "    data = data['T1'] + data['T2']\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, stratify = label, test_size = 0.2, random_state = 1234)\n",
    "    \n",
    "    randomforest_model = RandomForestClassifier()\n",
    "\n",
    "    parameters = { 'max_depth' : [10, 20, 30],\n",
    "                   'n_estimators' : [90, 150, 180],\n",
    "                   'max_samples' : [0.6, 0.8]\n",
    "     }\n",
    "\n",
    "    cross_validation = 3\n",
    "    scoring_metric = \"f1\"\n",
    "#     clf = GridSearchCV(randomforest_model, \n",
    "#                                          parameters,\n",
    "#                                          cv = cross_validation,\n",
    "#                                          scoring = scoring_metric,\n",
    "#                                          return_train_score=True)\n",
    "    v = TfidfVectorizer()\n",
    "    \n",
    "    train_corpus = X_train\n",
    "    test_corpus = X_test\n",
    "    \n",
    "    train_vector = v.fit_transform(train_corpus)\n",
    "    test_vector = v.transform(test_corpus)\n",
    "    \n",
    "    \n",
    "#     fit = clf.fit(train_vector,y_train)\n",
    "#     print('Best Params ', clf.best_params_)\n",
    "    parameters = {\n",
    "     'max_depth' : 30,\n",
    "     'n_estimators' : 90,\n",
    "     'max_samples' : 0.8\n",
    "    }\n",
    "    clf = RandomForestClassifier(max_depth = parameters['max_depth'],\n",
    "                                            max_samples = parameters['max_samples'],\n",
    "                                            n_estimators = parameters['n_estimators'])\n",
    "\n",
    "    fit = clf.fit(train_vector,y_train)\n",
    "    pred = clf.predict(test_vector)\n",
    "    \n",
    "    \n",
    "    Accuracy = accuracy_score(y_test,pred)\n",
    "    F1 = f1_score(y_test, pred, average='macro')\n",
    "    print(\"Accuracy:\", Accuracy)\n",
    "    print('F1:', F1)\n",
    "    \n",
    "    rec = recall_score(y_test, pred, average='macro')\n",
    "    print('Recall: ', rec)\n",
    "    prec = precision_score(y_test, pred, average='macro')\n",
    "    print('Precision: ', prec)\n",
    "    \n",
    "    \n",
    "    return y_test, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5803048065650644\n",
      "F1: 0.5802350004398698\n",
      "Recall:  0.5802904860859144\n",
      "Precision:  0.5803390688259109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2166    1.0\n",
       " 4236    0.0\n",
       " 2001    0.0\n",
       " 512     1.0\n",
       " 3240    0.0\n",
       "        ... \n",
       " 747     1.0\n",
       " 535     1.0\n",
       " 311     1.0\n",
       " 750     1.0\n",
       " 2121    0.0\n",
       " Name: class, Length: 853, dtype: float64,\n",
       " array([1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 1.]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(balanced_p1[['T1', 'T2']], balanced_p1['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8995367689635205\n",
      "F1: 0.5611401864573291\n",
      "Recall:  0.5481696174719145\n",
      "Precision:  0.9465625889029036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2269     0.0\n",
       " 29062    0.0\n",
       " 21144    0.0\n",
       " 64101    1.0\n",
       " 46390    0.0\n",
       "         ... \n",
       " 46736    0.0\n",
       " 47506    0.0\n",
       " 49566    0.0\n",
       " 186      1.0\n",
       " 6185     0.0\n",
       " Name: class, Length: 17270, dtype: float64,\n",
       " array([0., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(binary_random[['T1', 'T2']], binary_random['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task P1: Balanced\n",
      "╒══════════════════════════╤══════════╤═══════════╤════════╤══════════╕\n",
      "│ Model                    │ Accuracy │ Precision │ Recall │ F1 Score │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ Logistic Regression      │ 50.2     │ 50.2      │ 50.2   │ 49.8     │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ Random Forest Classifier │ 58.0     │ 58.0      │ 58.0   │ 58.0     │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ XGBoost Classifier       │ 69.9     │ 69.9      │ 69.9   │ 69.9     │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ Naive Bayes Classifier   │ 36.1     │ 35.7      │ 36.1   │ 35.7     │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ SVM Classifier           │ 52       │ 53.9      │ 52.1   │ 45.7     │\n",
      "╘══════════════════════════╧══════════╧═══════════╧════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "print('Task P1: Balanced')\n",
    "conclusion = [['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score',],\n",
    "             ['Logistic Regression', 50.2, 50.2, 50.2, 49.8],\n",
    "             ['Random Forest Classifier', 58.0, 58.0, 58.0, 58.0],\n",
    "             ['XGBoost Classifier',  69.9, 69.9, 69.9, 69.9],\n",
    "              ['Naive Bayes Classifier',  36.1, 35.7, 36.1, 35.7],\n",
    "              ['SVM Classifier',  52, 53.9, 52.1, 45.7],\n",
    "             ]\n",
    "print(tabulate.tabulate(conclusion, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task P1: Binary Random\n",
      "╒══════════════════════════╤══════════╤═══════════╤════════╤══════════╕\n",
      "│ Model                    │ Accuracy │ Precision │ Recall │ F1 Score │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ Logistic Regression      │ 88.9     │ 44.4      │ 50     │ 47       │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ Random Forest Classifier │ 89.9     │ 94.6      │ 54.8   │ 56.1     │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ XGBoost Classifier       │ 92.7     │ 91.4      │ 69.2   │ 75.2     │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ Naive Bayes Classifier   │ 63.6     │ 50.8      │ 51.8   │ 47.4     │\n",
      "├──────────────────────────┼──────────┼───────────┼────────┼──────────┤\n",
      "│ SVM Classifier           │ 75.3     │ 49.8      │ 49.6   │ 49.3     │\n",
      "╘══════════════════════════╧══════════╧═══════════╧════════╧══════════╛\n"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "print('Task P1: Binary Random')\n",
    "conclusion = [['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score',],\n",
    "             ['Logistic Regression', 88.9, 44.4, 50, 47],\n",
    "             ['Random Forest Classifier', 89.9, 94.6, 54.8, 56.1],\n",
    "             ['XGBoost Classifier',  92.7, 91.4, 69.2, 75.2],\n",
    "              ['Naive Bayes Classifier',  63.6, 50.8, 51.8, 47.4],\n",
    "              ['SVM Classifier',  75.3, 49.8, 49.6, 49.3],\n",
    "             ]\n",
    "print(tabulate.tabulate(conclusion, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.8995367689635205\n",
    "F1: 0.5611401864573291\n",
    "Recall:  0.5481696174719145\n",
    "Precision:  0.9465625889029036"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
